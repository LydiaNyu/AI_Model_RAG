import json
import os
import sys
import time
import logging
from datetime import datetime
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from rag_engine import ResearchRAGEngine

# Configure logging for the eval run
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("eval_runner")

# ==========================================
# 20 Research Questions (Phase 1 & 2 Aligned)
# ==========================================
EVAL_QUESTIONS = [
    # --- Group A: Productivity Claims (The "Gains") ---
    "How much faster did the treatment group complete the task in the Peng2023 study?",
    "What did Weber2024 find regarding the impact of AI on task completion time?",
    "Does GitHub Copilot significantly improve task success rates according to Vaithilingam2022?",
    "What is the 'bimodal' interaction pattern (Acceleration vs Exploration) described in Barke2023?",
    "According to Liang2024, what are the top reasons developers choose to use AI tools?",

    # --- Group B: Security & Quality Risks (The "Costs") ---
    "What percentage of code generated by Copilot contained vulnerabilities in the Pearce2022 study?",
    "Does Perry2023 find that users write more secure or less secure code with AI assistants?",
    "According to Kabir2024, what percentage of ChatGPT answers contained incorrect information?",
    "Did Yetistiren2023 find that AI-generated code is more maintainable than human code?",
    "What specific security vulnerabilities (CWEs) were most common in Pearce2022's analysis?",

    # --- Group C: Cognitive Skill Decay (The Phase 1 Core Question) ---
    "How does the use of Generative AI affect student frustration levels according to Choudhuri2024?",
    "What is the 'illusion of competence' or metacognitive difficulty described in Prather2023?",
    "Does Hashmi2024 suggest that AI increases or decreases student confidence?",
    "What are the 'cognitive costs' of validating AI code mentioned in Mozannar2024?",
    "Does the corpus provide evidence that relying on AI leads to a decline in problem-solving skills?",

    # --- Group D: Synthesis & Edge Cases (Complex Reasoning) ---
    "Compare the findings of Peng2023 and Perry2023 regarding the trade-off between speed and security.",
    "Do Liang2024 and Vaithilingam2022 agree on why developers struggle with AI tools?",
    "What does the corpus say about the impact of AI on 'Code Churn'?",
    "Does the corpus contain evidence about the impact of AI on cooking recipes? (Edge Case)",
    "What is the impact of AI on C++ specifically versus Python? (Edge Case)"
]

def run_evaluation():
    print("============================================================")
    print("STARTING PHASE 2 EVALUATION RUN (20 Questions)")
    print("============================================================")

    # 1. Initialize RAG Engine
    try:
        engine = ResearchRAGEngine(use_reranker=True)
        print("✅ RAG Engine initialized.")
        print(f"   - Model: {engine.model_name}")
        print(f"   - Documents: {engine.vectorstore._collection.count()}")
    except Exception as e:
        print(f"❌ Failed to initialize RAG Engine: {e}")
        return

    results = []
    # Use absolute path for logs directory
    output_dir = Path(__file__).parent.parent / "logs"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "evaluation_results.json"

    # 2. Loop through questions
    for i, question in enumerate(EVAL_QUESTIONS, 1):
        print(f"\nProcessing Q{i}/{len(EVAL_QUESTIONS)}: {question}")
        
        start_time = time.time()
        try:
            # Call the query method
            response = engine.query(question) 
            
            # Extract data safely
            if isinstance(response, dict):
                answer_text = response.get('answer', "No answer key found")
                sources_list = response.get('sources', [])
                # Use the new convenience field if available, otherwise extract from sources
                source_ids = response.get('source_ids', [])
                if not source_ids:
                    source_ids = [s.get('source_id', 'Unknown') for s in sources_list]
            else:
                answer_text = str(response)
                sources_list = []
                source_ids = []
            
            # Build detailed chunks for auditability (matches new format)
            retrieved_chunks = []
            for s in sources_list:
                chunk_info = {
                    "source_id": s.get('source_id', 'Unknown'),
                    "year": s.get('year'),
                    "page": s.get('page'),
                    "content_snippet": s.get('content_snippet', 'N/A')
                }
                retrieved_chunks.append(chunk_info)

            entry = {
                "question_id": i,
                "question": question,
                "timestamp": datetime.now().isoformat(),
                "answer": answer_text,
                # Simple list for quick reference
                "retrieved_source_ids": list(set(source_ids)),
                # Detailed chunks for auditability
                "retrieved_chunks": retrieved_chunks,
                "latency_seconds": round(time.time() - start_time, 2)
            }
            results.append(entry)
            print(f"   -> [Success] Sources: {list(set(source_ids))}")

        except Exception as e:
            logger.error(f"Failed to process query '{question}': {e}")
            results.append({
                "question_id": i,
                "question": question,
                "error": str(e)
            })

    # 3. Save Results
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)

    print("\n============================================================")
    print(f"✅ Evaluation complete. Results saved to {output_file}")
    print(f"   - Total questions: {len(EVAL_QUESTIONS)}")
    print(f"   - Successful: {sum(1 for r in results if 'error' not in r)}")
    print(f"   - Failed: {sum(1 for r in results if 'error' in r)}")
    print("============================================================")


if __name__ == "__main__":
    run_evaluation()